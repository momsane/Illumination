#wd=/work/FAC/FBM/DMF/pengel/general_data/20241106_Illumina_genomes/workflow/scripts
#command to symlink scratch: ln -s /scratch/mgarci14 ../../results/scratch
#workflow = scripts, envs, config
#data
#results
#logs
#benchmarks

configfile: "../config/config.yaml"
metadata_file = "../config/metadata.tsv"

# Import packages
import numpy as np
import pandas as pd

#Read config file
metadata = pd.read_table(metadata_file, sep='\t', header=0).set_index('sample', drop = False)

#List samples to include for the read pre-processing and assembly
SAMPLES_ASS = metadata['sample'].tolist()

#List samples to include for the downstream genome analysis
SAMPLES_ANNOT = metadata.loc[metadata['include_downstream'] == 1, 'sample'].tolist()

rule all:
    input:
        # First chunk: Illumina reads QC
        "../../results/preprocessing/check_integrity/integrity_files.txt",
        "../../results/preprocessing/count_reads_bt/count_reads_bt.txt",
        "../../results/preprocessing/fastqc_pretrim/all",
        # Second chunk: adapter trimming and assembly
        "../../results/preprocessing/fastqc_posttrim/all",
        "../../results/preprocessing/read_loss.txt",
        "../../results/assembly/QC/multiquast",
        expand("../../results/assembly/spades/contigs_filtered/{sample}.fna", sample = SAMPLES_ASS),
        expand("../../results/assembly/assemblies_noname/{sample}.fna", sample = SAMPLES_ASS),
        "../../results/checkm",
        # Third chunk: annotation
        "../../results/checkm_plots",
        "../../results/gtdbtk_classify",
        expand("../../results/annotation/DRAM/{sample}_DRAM", sample = SAMPLES_ANNOT),
        expand("../../results/annotation/rRNAS/{sample}_rrnas.fna", sample = SAMPLES_ANNOT), # comment this line after requesting all inputs to avoid rerunning on samples for which rRNAs cannot be found
        expand("../../results/annotation/RGI/{sample}", sample = SAMPLES_ANNOT),
        expand("../../results/annotation/macsyfinder/{sample}", sample = SAMPLES_ANNOT),
        "../../results/analysis/combined_gtdbtk_checkm.tsv"

# The pipeline assumes the reads from different lanes are already
# concatenated into a single file 

# File names should be something like: {sample}_R1.fastq.gz for Illumina and {sample}.fastq.gz for ONT

############################ Reads processing ############################

# First we QC the Illumina reads

# Check that all raw read files are complete

rule gzip_test:
    input:
        R1 = config["Illumina_raw"] + "/{sample}_R1.fastq.gz",
        R2 = config["Illumina_raw"] + "/{sample}_R2.fastq.gz",
    output:
        "../../results/preprocessing/check_integrity/samples/{sample}.txt"
    log:
        "../../logs/preprocessing/check_integrity/samples/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "20m",
        mem_mb = 500
    shell:
        """
        if gzip -t {input.R1}; then
            echo {input.R1}": OK" > {output}
        else 
            echo {input.R1}": Inappropriate file type or format" > {output}
        fi
        if gzip -t {input.R2}; then
            echo {input.R2}": OK" >> {output}
        else 
            echo {input.R2}": Inappropriate file type or format" >> {output}
        fi
        """

# Incomplete/empty files are now printed to one file.
# Check the output of the following rule

rule check_integrity:
    input:
        expand("../../results/preprocessing/check_integrity/samples/{sample}.txt", sample = SAMPLES_ASS)
    output:
        "../../results/preprocessing/check_integrity/integrity_files.txt"
    log:
        "../../logs/preprocessing/check_integrity/all.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "10m",
        mem_mb = 100
    shell:
        """
        echo {input} | xargs cat >> {output}
        """

# Count number of reads in each fastq file before trimming.

rule count_reads_bt:
    input:
        R1 = config["Illumina_raw"] + "/{sample}_R1.fastq.gz",
        R2 = config["Illumina_raw"] + "/{sample}_R2.fastq.gz",
    output:
        "../../results/preprocessing/count_reads_bt/samples/count_reads_bt_{sample}.txt"
    params:
        tmp = "../../results/preprocessing/count_reads_bt/samples/count_reads_bt_{sample}.temp.txt"
    conda:
        "../envs/seqkit-2.6.1.yaml"
    log:
        "../../logs/preprocessing/count_reads_bt/samples/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "1h",
        mem_mb = 2000
    shell:
        """ 
        seqkit stats -T -b -e {input.R1} > {params.tmp};
        seqkit stats -T -b -e {input.R2} >> {params.tmp};
        cat {params.tmp} | awk -v sample={wildcards.sample} 'NR == 2 {{print sample"\tR1\t"$0}}' > {output};
        cat {params.tmp} | awk -v sample={wildcards.sample} 'NR == 2 {{print sample"\tR2\t"$0}}' >> {output};
        rm {params.tmp}
        """

# Combining all the samples stats into one file 

rule combine_readcounts_bt:
    input:
        expand("../../results/preprocessing/count_reads_bt/samples/count_reads_bt_{sample}.txt", sample = SAMPLES_ASS)
    output:
        "../../results/preprocessing/count_reads_bt/count_reads_bt.txt"
    log:
        "../../logs/preprocessing/count_reads_bt/combine_readcounts.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "30m",
        mem_mb = 1000
    shell:
        """
        echo -e "sample\tread\tfile\tformat\ttype\tnum_seqs\tsum_len\tmin_len\tavg_len\tmax_len" > {output};
        echo {input} | xargs cat >> {output}
        """

rule fastqc_bt:
    input:
        R1 = config["Illumina_raw"] + "/{sample}_R1.fastq.gz",
        R2 = config["Illumina_raw"] + "/{sample}_R2.fastq.gz",
    output:
        directory("../../results/preprocessing/fastqc_pretrim/samples/{sample}")
    conda:
        "../envs/fastqc-0.11.8.yaml"
    log:
        "../../logs/preprocessing/fastqc_pretrim/samples/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "1h",
        mem_mb = 1000
    shell:
        """
        mkdir -p {output};
        fastqc -o {output} {input.R1} {input.R2}
        """

# Compile and visualize all fastqc reports together

rule multiqc_fastqc_bt:
    input:
        expand("../../results/preprocessing/fastqc_pretrim/samples/{sample}", sample = SAMPLES_ASS)
    output:
        directory("../../results/preprocessing/fastqc_pretrim/all")
    conda:
        "../envs/multiqc-1.6.yaml"
    params:
        ignore = ".zip"
    log:
        "../../logs/preprocessing/fastqc_pretrim/multiqc.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "30m",
        mem_mb = 2000
    shell:
        "multiqc --interactive -f --ignore {params.ignore} -o {output} {input}"

# The metadata file is updated to indicate the adapter content and read length
# Then you need to upload the correct adapter sequence(s) to the data folder

### Trimming

def determine_adapters(wildcards):
    adapt=metadata.loc[metadata['sample'] == wildcards.sample, 'adapter'].item()
    if (adapt=='NT'):
        return '../../data/adapters/Nextera_Transposase.fa'
    elif (adapt=='IU'):
        return '../../data/adapters/Illumina_Universal.fa'
    elif (adapt=='TS'):
        return '../../data/adapters/TruSeq.fa'

def determine_k(wildcards):
    adapt=metadata.loc[metadata['sample'] == wildcards.sample, 'adapter'].item()
    if (adapt=='NT'):
        return 31 #maximum allowed by bbduk
    elif (adapt=='IU'):
        return 19
    elif (adapt=='TS'):
        return 31 

# hdist is set to 2 to allow for 2 mismatches in the adapter sequence
# which more stringent than the default of 1

rule bbduk_adapt:
    input:
        R1 = config["Illumina_raw"] + "/{sample}_R1.fastq.gz",
        R2 = config["Illumina_raw"] + "/{sample}_R2.fastq.gz",
    output:
        trim1 = "../../results/preprocessing/trimmed_reads/{sample}_R1.trim.fastq.gz",
        trim2 = "../../results/preprocessing/trimmed_reads/{sample}_R2.trim.fastq.gz"
    params:
        adapt = determine_adapters,
        k = determine_k,
        mink = 11,
        hdist = 2,
        hdist2 = 0,
        overlap = 12,
        minlen = 40,
        q = 26
    log:
        "../../logs/preprocessing/read_trimming/{sample}_trimming"
    threads: 2
    conda:
        "../envs/bbmap-39.01.yaml"
    resources:
        account = "pengel_general_data",
        runtime = "1h",
        mem_mb = 2000
    shell:
        """
        bbduk.sh in={input.R1} in2={input.R2} out={output.trim1} out2={output.trim2} \
        ref={params.adapt} ktrim=r k={params.k} mink={params.mink} \
        hdist={params.hdist} hdist2={params.hdist2} \
        tpe=t tbo=t minoverlap={params.overlap} minlen={params.minlen} rcomp=f \
        qtrim=rl trimq={params.q}
        """

# Run fastqc after trimming

rule fastqc_pt:
    input:
        R1 = "../../results/preprocessing/trimmed_reads/{sample}_R1.trim.fastq.gz",
        R2 = "../../results/preprocessing/trimmed_reads/{sample}_R2.trim.fastq.gz"
    output:
        directory("../../results/preprocessing/fastqc_posttrim/samples/{sample}")
    conda:
        "../envs/fastqc-0.11.8.yaml"
    log:
        "../../logs/preprocessing/fastqc_posttrim/samples/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "1h",
        mem_mb = 1000
    shell:
        """
        mkdir -p {output};
        fastqc -o {output} {input.R1} {input.R2}
        """

# Compile and visualize all fastqc reports together

rule multiqc_fastqc_pt:
    input:
        expand("../../results/preprocessing/fastqc_posttrim/samples/{sample}", sample = SAMPLES_ASS)
    output:
        directory("../../results/preprocessing/fastqc_posttrim/all")
    conda:
        "../envs/multiqc-1.6.yaml"
    params:
        ignore = ".zip"
    log:
        "../../logs/preprocessing/fastqc_posttrim/multiqc.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "30m",
        mem_mb = 2000
    shell:
        "multiqc --interactive -f --ignore {params.ignore} -o {output} {input}"

# Count number of reads in each fastq file after trimming.

rule count_reads_pt:
    input:
        R1 = "../../results/preprocessing/trimmed_reads/{sample}_R1.trim.fastq.gz",
        R2 = "../../results/preprocessing/trimmed_reads/{sample}_R2.trim.fastq.gz"
    output:
        tmp = temp("../../results/preprocessing/count_reads_pt/samples/count_reads_pt_{sample}.temp.txt"),
        final = "../../results/preprocessing/count_reads_pt/samples/count_reads_pt_{sample}.txt"
    conda:
        "../envs/seqkit-2.6.1.yaml"
    log:
        "../../logs/preprocessing/count_reads_pt/samples/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "1h",
        mem_mb = 2000
    shell:
        """ 
        seqkit stats -T -b -e {input.R1} > {output.tmp};
        seqkit stats -T -b -e {input.R2} >> {output.tmp};
        cat {output.tmp} | awk -v sample={wildcards.sample} 'NR == 2 {{print sample"\tR1\t"$0}}' > {output.final};
        cat {output.tmp} | awk -v sample={wildcards.sample} 'NR == 2 {{print sample"\tR2\t"$0}}' >> {output.final}
        """

# Combining all the samples stats into one file 

rule combine_readcounts_pt:
    input:
        expand("../../results/preprocessing/count_reads_pt/samples/count_reads_pt_{sample}.txt", sample = SAMPLES_ASS)
    output:
        "../../results/preprocessing/count_reads_pt/count_reads_pt.txt"
    log:
        "../../logs/preprocessing/count_reads_pt/combine_readcounts.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "30m",
        mem_mb = 1000
    shell:
        """
        echo -e "sample\tread\tfile\tformat\ttype\tnum_seqs\tsum_len\tmin_len\tavg_len\tmax_len" > {output};
        echo {input} | xargs cat >> {output}
        """

# Compute number of reads lost during trimming

rule loss_reads:
    input:
        bt = "../../results/preprocessing/count_reads_bt/count_reads_bt.txt",
        pt = "../../results/preprocessing/count_reads_pt/count_reads_pt.txt"
    output:
        "../../results/preprocessing/read_loss.txt"
    log:
        "../../logs/preprocessing/read_loss.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "10m",
        mem_mb = 1000
    run:
        bt = pd.read_table(input.bt, sep='\t', header=0, index_col=None).drop('file', axis=1)
        pt = pd.read_table(input.pt, sep='\t', header=0, index_col=None).drop('file', axis=1)
        bt['set'] = bt['sample'] + bt['read']
        pt['set'] = pt['sample'] + pt['read']
        bt = bt.rename(columns={'num_seqs': 'num_seqs_bt', 'sum_len': 'sum_len_bt', 'min_len': 'min_len_bt', 'avg_len': 'avg_len_bt', 'max_len': 'max_len_bt'})
        pt = pt.rename(columns={'num_seqs': 'num_seqs_pt', 'sum_len': 'sum_len_pt', 'min_len': 'min_len_pt', 'avg_len': 'avg_len_pt', 'max_len': 'max_len_pt'})
        tot = pt.merge(bt, how = 'left', on = 'set')
        tot.drop('set', axis=1)
        tot['loss_seqs'] = tot['num_seqs_bt'] - tot['num_seqs_pt']
        tot['loss_len'] = tot['sum_len_bt'] - tot['sum_len_pt']
        tot['loss_seqs_perc'] = 100*tot['loss_seqs']/tot['num_seqs_bt']
        tot['loss_len_perc'] = 100*tot['loss_len']/tot['sum_len_bt']
        tot.to_csv(output[0], sep='\t', index=True)
        

############################ Assembly ############################

rule spades:
    input:
        R1 = "../../results/preprocessing/trimmed_reads/{sample}_R1.trim.fastq.gz",
        R2 = "../../results/preprocessing/trimmed_reads/{sample}_R2.trim.fastq.gz"
    output:
        contigs = "../../results/assembly/spades/contigs/{sample}_contigs.fasta",
        graph = "../../results/assembly/spades/graphs/{sample}_graph.fastg",
        log = "../../results/assembly/spades/logs/{sample}_log.txt"
    params:
        outdir = directory("../../results/scratch/spades"),
        fcont = directory("../../results/assembly/spades/contigs"),
        flog = directory("../../results/assembly/spades/logs"),
        fgraph = directory("../../results/assembly/spades/graphs")
    conda:
        "../envs/spades-3.15.5.yaml"
    log:
        "../../logs/spades/{sample}.log"
    threads: 24
    resources:
        account = "pengel_general_data",
        runtime = "8h",
        mem_mb = 40000
    shell:
        """
        mkdir -p {params.outdir};
        mkdir -p {params.fcont};
        mkdir -p {params.flog};
        mkdir -p {params.fgraph};
        spades.py --isolate --pe1-1 {input.R1} --pe1-2 {input.R2} \
        -o {params.outdir}/{wildcards.sample}_assembly -t {threads};
        cp {params.outdir}/{wildcards.sample}_assembly/contigs.fasta {output.contigs};
        cp {params.outdir}/{wildcards.sample}_assembly/assembly_graph.fastg {output.graph};
        cp {params.outdir}/{wildcards.sample}_assembly/spades.log {output.log}
        """

# QC of assemblies using QUAST. Minimum contig length to include it is 500 bp (default for QUAST).

rule quast:
    input:
        "../../results/assembly/spades/contigs/{sample}_contigs.fasta"
    output:
        directory("../../results/assembly/QC/quast/{sample}_quast")
    log:
        "../../logs/quast/{sample}.log"
    conda:
        "../envs/quast-5.2.0.yaml"
    threads: 4
    resources:
        account = "pengel_general_data",
        runtime = "4h",
        mem_mb = 40000
    shell:
        """
        quast.py --no-snps --no-sv --memory-efficient -o {output} -t {threads} {input}
        """

# Aggregate results

rule multiqc_metaquast:
    input:
        expand("../../results/assembly/QC/quast/{sample}_quast", sample = SAMPLES_ASS)
    output:
        directory("../../results/assembly/QC/multiquast")
    conda:
        "../envs/multiqc-1.6.yaml"
    log:
        "../../logs/assembly/multiqc_quast/multiqc.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "30m",
        mem_mb = 2000
    shell:
        "multiqc --interactive -f -o {output} {input}"
        
# Next we want to:
# (1) filter contigs to retain only the ones > 1000 bp and with a minimum read coverage of 1
# (2) rename contigs like so: sample_contignumber_length.

rule filter_rename_contigs:
    input:
        contigs = "../../results/assembly/spades/contigs/{sample}_contigs.fasta",
        awk_script = "filter_contigs.awk"
    output:
        keep = "../../results/assembly/spades/contigs_filtered/{sample}.fna",
        discard = "../../results/assembly/spades/contigs_discarded/{sample}_contigs_discarded.fna",
        stats = "../../results/assembly/spades/stats/{sample}_contig_stats.txt"
    params:
        dirk = "../../results/assembly/spades/contigs_filtered",
        dird = "../../results/assembly/spades/contigs_discarded",
        cp = "../../results/assembly/contigs_filtered"
    log:
        "../../logs/filter_contigs/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "30m",
        mem_mb = 1000
    shell:
        """
        mkdir -p {params.dirk};
        mkdir -p {params.dird};
        mkdir -p {params.cp};
        awk -v sample={wildcards.sample} -v fk={output.keep} -v fd={output.discard} -v fs={output.stats} \
        -f {input.awk_script} {input.contigs};
        cp {output.keep} {params.cp}/{wildcards.sample}_contigs_filtered.fasta
        """

# A second version of the assembly with simplified fasta headers and 
# without the sample name because DRAM will add it itself

rule rename_contigs_noname:
    input:
        contigs = "../../results/assembly/spades/contigs_filtered/{sample}.fna",
        awk_script = "filter_contigs_noname.awk"
    output:
        "../../results/assembly/assemblies_noname/{sample}.fna"
    params:
        fld = "../../results/assembly/assemblies_noname"
    log:
        "../../logs/filter_contigs_noname/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        runtime = "30m",
        mem_mb = 1000
    shell:
        """
        mkdir -p {params.fld};
        awk -f {input.awk_script} {input.contigs} > {output}
        """
        
# Next we run checkM to assess the quality of the assemblies

rule checkm:
    input:
        expand("../../results/assembly/spades/contigs_filtered/{sample}.fna", sample = SAMPLES_ASS)
    output:
        fld = directory("../../results/checkm"),
        rextended = "../../results/checkm/checkm_report_extended.txt"
    params:
        fld = "../../results/assembly/spades/contigs_filtered",
        extension = "fna",
        tmpdir = "../../results/scratch/checkm.tmp",
        tmprep = "../../results/checkm/checkm_report_simple.txt",
        lineage = "../../results/checkm/lineage.ms"
    log:
        "../../logs/checkm/checkm.log"
    conda:
        "../envs/checkm-1.2.2.yaml"
    threads: 2
    resources:
        account = "pengel_general_data",
        mem_mb = 100000,
        runtime = "24h"
    shell:
        """
        mkdir -p {params.tmpdir};
        checkm lineage_wf -f {params.tmprep} --tab_table -x {params.extension} \
        -t {threads} --tmpdir {params.tmpdir} {params.fld} {output.fld};
        checkm qa -o 2 -f {output.rextended} --tab_table --tmpdir {params.tmpdir} \
        -t {threads} {params.lineage} {output.fld};
        rm {params.tmprep};
        rm -rf {params.tmpdir}
        """

# Check the checkM output to see if the genomes seem complete
# From here only complete genomes will be further processed

############################ Classification & annotation ##########################################

rule checkm_plots:
    input:
        assemblies = expand("../../results/assembly/spades/contigs_filtered/{sample}.fna", sample = SAMPLES_ANNOT),
        dir_checkm = "../../results/checkm"
    output:
        fld = directory("../../results/checkm_plots")
    params:
        dir_asmbl = "../../results/assembly/spades/contigs_filtered",
        extension = "fna"
    benchmark:
        "../../benchmarks/checkm_plots.benchmark"
    log:
        "../../logs/checkm/plot_checkm.log"
    threads: 1
    conda:
        "../envs/checkm-1.2.2.yaml"
    resources:
        account = "pengel_general_data",
        mem_mb = 2000,
        runtime= "20m"
    shell:
        """
        checkm marker_plot {input.dir_checkm} {params.dir_asmbl} {output.fld} -x {params.extension} --image_type svg;
        checkm coding_plot {input.dir_checkm} {params.dir_asmbl} {output.fld} 0 -x {params.extension} --image_type svg;
        checkm gc_plot {params.dir_asmbl} {output.fld} 0 -x {params.extension} --image_type svg;
        checkm nx_plot {params.dir_asmbl} {output.fld} -x {params.extension} --image_type svg
        """

rule gtdb_classify:
    input:
        expand("../../results/assembly/spades/contigs_filtered/{sample}.fna", sample = SAMPLES_ANNOT)
    output:
        fld = directory("../../results/gtdbtk_classify"),
        report = "../../results/gtdbtk_classify/classify/gtdbtk.bac120.summary.tsv"
    log:
        "../../logs/gtdbtk/gtdbtk_classify"
    benchmark:
        "../../benchmarks/gtdbtk_classify.benchmark"
    params:
        fld = "../../results/assembly/spades/contigs_filtered",
        extension = "fna",
        mashdb = config["Mash_db"],
        tmpdir = "../../results/scratch/gtdbtk_classify/",
        scratchdir = "../../results/scratch/gtdbtk_classify_pplacer"
    conda:
        "../envs/gtdbtk-2.4.0.yaml"
    threads: 4
    resources:
        account = "pengel_general_data",
        mem_mb = 20000,
        runtime = "24h"
    shell:
        """
        mkdir -p {params.tmpdir};
        gtdbtk classify_wf --genome_dir {params.fld} --mash_db {params.mashdb} --scratch_dir {params.scratchdir} \
        --out_dir {output.fld} --extension {params.extension} --cpus {threads} --tmpdir {params.tmpdir}
        """

rule DRAM:
    input:
        asmbl = "../../results/assembly/assemblies_noname/{sample}.fna",
        checkm = "../../results/checkm/checkm_report_extended.txt",
        gtdbtk = "../../results/gtdbtk_classify/classify/gtdbtk.bac120.summary.tsv",
        config = "../config/dram-1.4.6_config.txt"
    output:
        fld1 = directory("../../results/annotation/DRAM/{sample}_DRAM"),
        fld2 = directory("../../results/annotation/DRAM/{sample}_DRAM/distill"),
        annot = "../../results/annotation/DRAM/{sample}_DRAM/annotations.tsv",
        faa = "../../results/annotation/DRAM/{sample}_DRAM/genes.faa",
        fna = "../../results/annotation/DRAM/{sample}_DRAM/genes.fna",
        gbk = "../../results/annotation/DRAM/{sample}_DRAM/genbank/{sample}.gbk",
        scaff = "../../results/annotation/DRAM/{sample}_DRAM/scaffolds.fna",
        gff = "../../results/annotation/DRAM/{sample}_DRAM/genes.gff",
        met = "../../results/annotation/DRAM/{sample}_DRAM/distill/metabolism_summary.xlsx",
        prod1 = "../../results/annotation/DRAM/{sample}_DRAM/distill/product.html",
        prod2 = "../../results/annotation/DRAM/{sample}_DRAM/distill/product.tsv",
        stats = "../../results/annotation/DRAM/{sample}_DRAM/distill/genome_stats.tsv"
    params:
        trnas = "../../results/annotation/DRAM/{sample}_DRAM/trnas.tsv",
        rrnas = "../../results/annotation/DRAM/{sample}_DRAM/rrnas.tsv"
    log:
        "../../logs/annotation/DRAM/{sample}_dram.log"
    conda:
        "../envs/dram-1.4.6.yaml"
    threads: 8
    resources:
        account = "pengel_general_data",
        mem_mb = 30000,
        runtime= "6h"
    shell:
        """
        rm -rf {output.fld1};
        (DRAM-setup.py import_config --config_loc {input.config}) 2> {log};
        (DRAM.py annotate -i {input.asmbl} -o {output.fld1} --min_contig_size 999 \
        --checkm_quality {input.checkm} --gtdb_taxonomy {input.gtdbtk} --threads {threads} --verbose) 2> {log};
        if [[ -f {params.trnas} && -f {params.rrnas} ]]; then
            (DRAM.py distill -i {output.annot} -o {output.fld2} --trna_path {params.trnas} \
            --rrna_path {params.rrnas}) 2>> {log}
        elif [[ -f {params.trnas} && ! -f {params.rrnas} ]]; then
            (DRAM.py distill -i {output.annot} -o {output.fld2} --trna_path {params.trnas}) 2>> {log}
        elif [[ ! -f {params.trnas} && ! -f {params.rrnas} ]]; then
            (DRAM.py distill -i {output.annot} -o {output.fld2}) 2>> {log}
        fi
        """

rule extract_rrnas:
    input:
        "../../results/annotation/DRAM/{sample}_DRAM/scaffolds.fna"
    output:
        seqs = "../../results/annotation/rRNAS/{sample}_rrnas.fna",
        gff = "../../results/annotation/rRNAS/{sample}_rrnas.gff"
    params:
        fld = "../../results/annotation/rRNAS",
        king = "bac",
        r16s = "../../results/annotation/rRNAS/{sample}_16S.fna"
    log:
        "../../logs/annotation/barrnap/{sample}_barrnap.log"
    conda:
        "../envs/dram-1.4.6.yaml"
    threads: 1
    resources:
        account = "pengel_general_data",
        mem_mb = 2000,
        runtime= "1h"
    shell:
        """
        mkdir -p {params.fld};
        barrnap --threads {threads} --kingdom {params.king} --outseq {output.seqs} < {input} > {output.gff};
        sed -i "s/>/>{wildcards.sample}|/g" {output.seqs};
        if grep -Fq "16S" {output.seqs}
        then
            grep -A 1 "16S" {output.seqs} > {params.r16s}
        else
            echo "No 16S rRNA found"
        fi
        """

# remove all * from input fasta before passing it to rgi

rule rgi:
    input:
        "../../results/annotation/DRAM/{sample}_DRAM/genes.faa"
    output:
        fld = directory("../../results/annotation/RGI/{sample}"),
        raw = "../../results/annotation/RGI/{sample}/{sample}_RGI_raw.txt",
        filtered = "../../results/annotation/RGI/{sample}/{sample}_RGI_filtered.txt"
    params:
        tmp = "../../results/annotation/RGI/{sample}/{sample}.faa.tmp",
        db = "/work/FAC/FBM/DMF/pengel/general_data/mgarci14/RGI_db/card.json",
        bn = "../../results/annotation/RGI/{sample}/{sample}_RGI_raw"
    log:
        "../../logs/annotation/RGI/{sample}_rgi.log"
    conda:
        "../envs/rgi-6.0.3.yaml"
    threads: 1
    resources:
        account = "pengel_general_data",
        mem_mb = 4000,
        runtime= "1h"
    shell:
        """
        rgi load --card_json {params.db} --local;
        sed 's/*//g' {input} > {params.tmp}
        rgi main --input_sequence {params.tmp} --output_file {params.bn} --local --clean --include_loose -t protein -n {threads};
        awk -F '\t' '($10>40) && ($21>70) {{print}}' {output.raw} > {output.filtered};
        rm -rf {params.tmp}
        """

# If macsyfinder is installed with the latest version it works
# the conda environment also contains biopython

rule macsydata:
    output:
        fld = directory("../../results/annotation/macsyfinder/macsydata"),
        file = "../../results/annotation/macsyfinder/macsydata/macsydata.txt"
    log:
        "../../logs/annotation/macsyfinder/macsydata.log"
    conda:
        "../envs/macsyfinder-2.1.4.yaml"
    threads: 1
    resources:
        account = "pengel_general_data",
        mem_mb = 2000,
        runtime= "15m"
    shell:
        """
        mkdir -p {output.fld};
        mkdir -p ${{CONDA_PREFIX}}/share/macsyfinder/models/profiles;
        mkdir -p ${{CONDA_PREFIX}}/share/macsyfinder/models/definitions;
        mkdir -p ${{CONDA_PREFIX}}/share/macsyfinder/doc/profiles;
        mkdir -p ${{CONDA_PREFIX}}/share/macsyfinder/doc/definitions;
        macsydata install --target ${{CONDA_PREFIX}}/share/macsyfinder TXSScan;
        touch {output.file}
        """

# macsyfinder can run on both unordered and ordered genomes, but in the ordered mode more informative outputs are produced
# normally the genes.faa file from DRAM is already ordered
# if you start from a .faa file that did not come from DRAM (i.e. downloaded from NCBI), ensure the genes are correctly ordered

rule reorder_genes:
    input:
        data = "../../results/annotation/macsyfinder/macsydata/macsydata.txt",
        gff = "../../results/annotation/DRAM/{sample}_DRAM/genes.gff",
        faa = "../../results/annotation/DRAM/{sample}_DRAM/genes.faa"
    output:
        "../../results/annotation/macsyfinder/ordered_genomes/{sample}_ordered.faa"
    params:
        fld = "../../results/annotation/macsyfinder/ordered_genomes"
    conda:
        "../envs/macsyfinder-2.1.4.yaml"
    log:
        "../../logs/annotation/macsyfinder/order_genes/{sample}.log"
    threads: 1
    resources:
        account = "pengel_general_data",
        mem_mb = 2000,
        runtime= "1h"
    script:
        "order_genes.py"

rule macsyfinder:
    input:
        data = "../../results/annotation/macsyfinder/macsydata/macsydata.txt",
        faa = "../../results/annotation/macsyfinder/ordered_genomes/{sample}_ordered.faa"
    output:
        directory("../../results/annotation/macsyfinder/{sample}")
    log:
        "../../logs/annotation/macsyfinder/{sample}_df.log"
    conda:
        "../envs/macsyfinder-2.1.4.yaml"
    threads: 1
    resources:
        account = "pengel_general_data",
        mem_mb = 4000,
        runtime= "1h"
    shell:
        """
        macsyfinder --sequence-db {input.faa} -o {output} --models-dir ${{CONDA_PREFIX}}/share/macsyfinder --models TXSScan all --db-type ordered_replicon -w {threads} -vv
        """
        
## There are currently issues with macsyfinder so I cannot run defense-finder (it needs macsyfinder)

# rule defense_finder_update:
#     output:
#         fld = directory("../../results/annotation/defense-finder/update"),
#         file = "../../results/annotation/defense-finder/update/update.txt"
#     log:
#         "../../logs/annotation/defense-finder/update.log"
#     conda:
#         "../envs/defense-finder-1.3.0.yaml"
#     threads: 1
#     resources:
#         account = "pengel_general_data",
#         mem_mb = 4000,
#         runtime= "1h"
#     shell:
#         """
#         mkdir -p {output.fld};
#         defense-finder update;
#         touch {output.file}
#         """

# rule defense_finder:
#     input:
#         update = "../../results/annotation/defense-finder/update/update.txt",
#         faa = "../../results/annotation/DRAM/{sample}_DRAM/genes.faa"
#     output:
#         directory("../../results/annotation/defense-finder/{sample}")
#     log:
#         "../../logs/annotation/defense-finder/{sample}_df.log"
#     conda:
#         "../envs/defense-finder-1.3.0.yaml"
#     threads: 1
#     resources:
#         account = "pengel_general_data",
#         mem_mb = 4000,
#         runtime= "1h"
#     shell:
#         """
#         (defense-finder run {input.faa} --out-dir {output}) 2> {log}
#         """

rule combine_gtdbtk_checkm:
    input:
        checkm = "../../results/checkm/checkm_report_extended.txt",
        gtdbtk = "../../results/gtdbtk_classify/classify/gtdbtk.bac120.summary.tsv",
        script = "combine_gtdbtk_checkm.R"
    output:
        file = "../../results/analysis/combined_gtdbtk_checkm.tsv"
    log:
        "../../logs/combine_gtdbtk_checkm.log"
    conda:
        "../envs/R-4.3.2.yaml"
    threads: 1
    resources:
        account = "pengel_general_data",
        mem_mb = 500,
        runtime= "20m"
    script:
        "combine_gtdbtk_checkm.R"